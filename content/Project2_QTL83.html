---
title: 'Project 2: Modeling, Testing, and Predicting'
author: " Quynh Lam QTL83 SDS348 Fall 2019"
date: '11/26/19'
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
---



<div id="modeling" class="section level1">
<h1>Modeling</h1>
<div id="instructions" class="section level2">
<h2>Instructions</h2>
<p>A knitted R Markdown document (as a PDF) and the raw R Markdown file (as .Rmd) should both be submitted to Canvas by 11:59pm on 11/24/2019. These two documents will be graded jointly, so they must be consistent (i.e., don’t change the R Markdown file without also updating the knitted document). Knit an html copy too, for later! In the .Rmd file for Project 2, you can copy the first code-chunk into your project .Rmd file to get better formatting. Notice that you can adjust the opts_chunk$set(…) above to set certain parameters if necessary to make the knitting cleaner (you can globally set the size of all plots, etc). I have gone ahead and set a few for you (such as disabling warnings and package-loading messges when knitting)!</p>
<p>Like before, I envision your written text forming something of a narrative structure around your code/output. All results presented must have corresponding code. Any answers/results/plots etc. given without the corresponding R code that generated the result will not be graded. Furthermore, all code contained in your final project document should work properly. Please do not include any extraneous code or code which produces error messages. (Code which produces warnings is acceptable, as long as you understand what the warnings mean).</p>
</div>
<div id="find-data" class="section level2">
<h2>Find data:</h2>
<p>The dataset I will be using is called FaithfulFaces, where a rater will rate a photograph of a male or female on their attractiveness, trust worthiness, faithfulness, and sex dimorphia. The data also tells you if the person in the photograph is actually a cheater or not. Attractiveness, trustworthiness, faithfulness and sex dimorphia are all numerical variables. Cheater is a binary variable and tells you whether they are a cheater or not. The categorical variables are the face sex and ranker sex which tells you if they are male or female.</p>
</div>
<div id="manova-testing" class="section level2">
<h2>MANOVA Testing</h2>
<pre class="r"><code>man_faith &lt;- manova(cbind(SexDimorph,Attract,Trust,Faithful)~FaceSex,data=FaithfulFaces)
summary(man_faith)</code></pre>
<pre><code>## Df Pillai approx F num Df den Df Pr(&gt;F)
## FaceSex 1 0.20961 10.939 4 165 6.827e-08 ***
## Residuals 168
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>summary.aov(man_faith)</code></pre>
<pre><code>## Response SexDimorph :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## FaceSex 1 12.36 12.3598 13.885 0.0002652 ***
## Residuals 168 149.55 0.8902
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response Attract :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## FaceSex 1 0.363 0.36264 0.4798 0.4895
## Residuals 168 126.982 0.75584
##
## Response Trust :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## FaceSex 1 2.314 2.31372 3.7592 0.05419 .
## Residuals 168 103.401 0.61548
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response Faithful :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## FaceSex 1 27.229 27.2293 35.823 1.27e-08 ***
## Residuals 168 127.698 0.7601
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>pairwise.t.test(FaithfulFaces$Faithful,FaithfulFaces$FaceSex,p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  FaithfulFaces$Faithful and FaithfulFaces$FaceSex 
## 
##   F      
## M 1.3e-08
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(FaithfulFaces$SexDimorph,FaithfulFaces$FaceSex,p.adj=&quot;none&quot;)</code></pre>
<pre><code>##
## Pairwise comparisons using t tests with pooled SD
##
## data: FaithfulFaces$SexDimorph and FaithfulFaces$FaceSex
##
## F
## M 0.00027
##
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(FaithfulFaces$Trust,FaithfulFaces$FaceSex,p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  FaithfulFaces$Trust and FaithfulFaces$FaceSex 
## 
##   F    
## M 0.054
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>1-(.95)^16</code></pre>
<pre><code>## [1] 0.5598733</code></pre>
<pre class="r"><code>.05/16</code></pre>
<pre><code>## [1] 0.003125</code></pre>
<p>Running a manova to test to see if SexDimorph,Attractfulness, trustfulness, and faithfulness had a mean difference against face sex, it seems that there was as the p-value was below 0.05 and was 6.827e-08. After running an manova, an anova was ran to test to see which ones of the variables had a mean difference. Only SexDimorph,Trust, and Faithful were the only ones that had a mean difference while Attract did not as the p-value was 0.4895 and the other p-values were below 0.05. After the anova, a pairwise t test was run for SexDimorph,Trust, and Faithful since those were the only ones that showed to have a mean difference in the anova test. The pairwise t-test showed that all of those variables showed a difference between males and females, as they were below 0.05.</p>
</div>
<div id="randomized-testing" class="section level2">
<h2>Randomized Testing</h2>
<pre class="r"><code>FaithfulFaces%&gt;%group_by(FaceSex)%&gt;%summarize(mean_face=mean(Trust))</code></pre>
<pre><code>## # A tibble: 2 x 2
##   FaceSex mean_face
##   &lt;fct&gt;       &lt;dbl&gt;
## 1 F            4.44
## 2 M            4.21</code></pre>
<pre class="r"><code>4.444061-4.210591</code></pre>
<pre><code>## [1] 0.23347</code></pre>
<pre class="r"><code>rand_dist&lt;-vector()
for(i in 1:5000){
Faith&lt;-data.frame(sex=sample(FaithfulFaces$FaceSex),Trust=FaithfulFaces$Trust)
rand_dist[i]&lt;-mean(Faith[Faith$sex==&quot;F&quot;,]$Trust)-
mean(Faith[Faith$sex==&quot;M&quot;,]$Trust)}

mean(rand_dist&gt;0.23347)*2</code></pre>
<pre><code>## [1] 0.0556</code></pre>
<pre class="r"><code>{hist(rand_dist,main=&quot;&quot;,ylab=&quot;&quot;);abline(v=0.23347,col=&quot;red&quot;)}</code></pre>
<p><img src="/Project2_QTL83_files/figure-html/unnamed-chunk-2-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Using a randomized test, it took a sample of Face sex and pulled it 5,000 times and then taking the mean of females and males of Trust were subtracted to find the difference. After running a randomized test, I then found the p-value of a two-tailed test and it was less than 0.05, as it was 0.0564. I then plotted a histogram to show the null distribution and test statistic. The null hypothesis is that there is no difference in Trust in males and females, and the alternative hypothesis is that there is a difference in Trust in males and females. From the randomized test and finding the p-value, we can reject the null hypothesis.</p>
</div>
<div id="linear-regression" class="section level2">
<h2>Linear Regression</h2>
<pre class="r"><code>fit &lt;- lm(Attract~Trust*FaceSex, data=FaithfulFaces)
coef(fit)</code></pre>
<pre><code>## (Intercept) Trust FaceSexM Trust:FaceSexM
## 1.3648789 0.3555250 -0.2845297 0.1092399</code></pre>
<pre class="r"><code>interact_plot(fit,Trust,FaceSex,data=FaithfulFaces)</code></pre>
<p><img src="/Project2_QTL83_files/figure-html/unnamed-chunk-3-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#assumptions
resids&lt;-fit$residuals
fitvals&lt;-fit$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color=&#39;red&#39;)</code></pre>
<p><img src="/Project2_QTL83_files/figure-html/unnamed-chunk-3-2.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot()+geom_histogram(aes(resids), bins=20)</code></pre>
<p><img src="/Project2_QTL83_files/figure-html/unnamed-chunk-3-3.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot()+geom_qq(aes(sample=resids))+geom_qq_line()</code></pre>
<p><img src="/Project2_QTL83_files/figure-html/unnamed-chunk-3-4.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#robust standard errors
summary(fit)$coef[,1:2] #uncorrected</code></pre>
<pre><code>##                  Estimate Std. Error
## (Intercept)     1.3648789  0.5459844
## Trust           0.3555250  0.1212013
## FaceSexM       -0.2845297  0.7090092
## Trust:FaceSexM  0.1092399  0.1606560</code></pre>
<pre class="r"><code>coeftest(fit, vcov = vcovHC(fit))[,1:2] #corrected</code></pre>
<pre><code>##                  Estimate Std. Error
## (Intercept)     1.3648789  0.7469485
## Trust           0.3555250  0.1710796
## FaceSexM       -0.2845297  0.8565108
## Trust:FaceSexM  0.1092399  0.1969627</code></pre>
<p>After running a linear regression,the coefficent estimates for trust was 0.355250, Face Sex for males was -0.2845297 and the Trust:Face Sex M was 0.1092399. I then plotted Trust vs Attract in males and females and saw that there was a higher interaction in males than females. I then checked assumptions, and everything looked normal.</p>
</div>
<div id="bootstrapped-standard-error" class="section level2">
<h2>Bootstrapped Standard Error</h2>
<pre class="r"><code>boot_face&lt;-FaithfulFaces[sample(nrow(FaithfulFaces),replace=TRUE),]
samp_distn&lt;-replicate(5000, {
boot_face&lt;-FaithfulFaces[sample(nrow(FaithfulFaces),replace=TRUE),]
fit2&lt;-lm(Attract~Trust*FaceSex,data=FaithfulFaces)
coef(fit2)
})
samp_distn%&gt;%t%&gt;%as.data.frame%&gt;%summarize_all(sd)</code></pre>
<pre><code>##   (Intercept) Trust FaceSexM Trust:FaceSexM
## 1           0     0        0              0</code></pre>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic Regression</h2>
<pre class="r"><code>#logistic regression
fit2&lt;-glm(Cheater~Attract+Trust+Faithful, data=FaithfulFaces, family=&quot;binomial&quot;)
coeftest(fit2)</code></pre>
<pre><code>## 
## z test of coefficients:
## 
##             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)  0.88845    1.26872  0.7003   0.4838
## Attract     -0.18216    0.25986 -0.7010   0.4833
## Trust        0.11510    0.31059  0.3706   0.7109
## Faithful    -0.33650    0.24842 -1.3545   0.1756</code></pre>
<pre class="r"><code>#confusion matrix
prob &lt;- predict(fit2,type=&quot;response&quot;)
table(predict=as.numeric(prob&gt;.5),truth=FaithfulFaces$Cheater)%&gt;%addmargins</code></pre>
<pre><code>##        truth
## predict   0   1 Sum
##     0   120  50 170
##     Sum 120  50 170</code></pre>
<pre class="r"><code>class_diag&lt;-function(prob,truth){
tab&lt;-table(factor(prob&gt;.5,levels=c(&quot;FALSE&quot;,&quot;TRUE&quot;)),truth)
acc=sum(diag(tab))/sum(tab)
sens=tab[2,2]/colSums(tab)[2]
spec=tab[1,1]/colSums(tab)[1]
ppv=tab[2,2]/rowSums(tab)[2]
if(is.numeric(truth)==FALSE &amp; is.logical(truth)==FALSE) truth&lt;-as.numeric(truth)-1
#CALCULATE EXACT AUC
ord&lt;-order(prob, decreasing=TRUE)
prob &lt;- prob[ord]; truth &lt;- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup&lt;-c(prob[-1]&gt;=prob[-length(prob)], FALSE)
TPR&lt;-c(0,TPR[!dup],1); FPR&lt;-c(0,FPR[!dup],1)
n &lt;- length(TPR)
auc&lt;- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
data.frame(acc,sens,spec,ppv,auc)
}


class_diag(prob,FaithfulFaces$Cheater)</code></pre>
<pre><code>##         acc sens spec ppv       auc
## 1 0.7058824    0    1 NaN 0.5633333</code></pre>
<pre class="r"><code>#ROCplot
library(plotROC)

RocPlot &lt;- ggplot(FaithfulFaces)+geom_roc(aes(d=Cheater,m=prob),n.cut=0)
RocPlot</code></pre>
<p><img src="/Project2_QTL83_files/figure-html/unnamed-chunk-5-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>calc_auc(RocPlot)</code></pre>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.5633333</code></pre>
<pre class="r"><code>#10-fold
set.seed(1234)
k=10

data1&lt;-FaithfulFaces[sample(nrow(FaithfulFaces)),] #randomly order rows
folds&lt;-cut(seq(1:nrow(FaithfulFaces)),breaks=k,labels=FALSE) #create folds
diags&lt;-NULL
for(i in 1:k){
## Create training and test sets
train&lt;-data1[folds!=i,]
test&lt;-data1[folds==i,]
truth&lt;-test$Cheater
## Train model on training set
fit3&lt;-glm(Cheater~.,data=FaithfulFaces,family=&quot;binomial&quot;)
probs2&lt;-predict(fit3,newdata = test,type=&quot;response&quot;)
## Test model on test set (save all k results)
diags&lt;-rbind(diags,class_diag(probs2,truth))
}
apply(diags,2,mean)</code></pre>
<pre><code>##       acc      sens      spec       ppv       auc 
## 0.7000000 0.0000000 0.9909091       NaN 0.6046784</code></pre>
<p>A logistic regression was run and this time the intercepts tell us the interactions of attractfullness and trustfulness in a cheater. The intercepts for attract was -0.18216, trust was 0.11510, and faithful was -0.33650. The accuracy (0.7058824), sensitivity (0), specificity (1), and auc (0.5633333) was also computed. The auc is not as good since it is so low. After running the 10-fold, the auc seemed to stay the same and there was no difference.</p>
</div>
<div id="lasso" class="section level2">
<h2>LASSO</h2>
<pre class="r"><code>x &lt;- model.matrix(fit)[,-1]

y&lt;-as.matrix(FaithfulFaces$Cheater)

cv&lt;-cv.glmnet(x,y,family=&#39;binomial&#39;)

lasso1&lt;-glmnet(x,y,family=&#39;binomial&#39;,lambda=cv$lambda.1se)
coef(lasso1)</code></pre>
<pre><code>## 4 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                        s0
## (Intercept)    -0.8754687
## Trust           0.0000000
## FaceSexM        .        
## Trust:FaceSexM  .</code></pre>
<pre class="r"><code>set.seed(1234)
k=10

data2&lt;-FaithfulFaces[sample(nrow(FaithfulFaces)),]
folds&lt;-cut(seq(1:nrow(FaithfulFaces)),breaks=k,labels=F)
diags&lt;-NULL
for(i in 1:k){ # FOR EACH OF 10 FOLDS

train2&lt;-data2[folds!=i,] #CREATE TRAINING SET

test2&lt;-data2[folds==i,] #CREATE TESTING SET
truth2&lt;-test2$Cheater


probs3&lt;- predict(fit,newdata=test2, type=&quot;response&quot;)

diags&lt;-rbind(diags,class_diag(probs3,truth2)) 
}

apply(diags,2,mean)</code></pre>
<pre><code>##       acc      sens      spec       ppv       auc 
## 0.2941176 1.0000000 0.0000000 0.2941176 0.4658892</code></pre>
<p>A lasso regression was done and the auc was actually lower than then when I ran a logistic regression. The auc here is 0.4573660 and above was above 0.50. This was after running the 10-fold as well, and it seemed that the accuracy was not as high when running the logistic regression.</p>
</div>
</div>
