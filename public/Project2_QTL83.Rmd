---
title: 'Project 2: Modeling, Testing, and Predicting'
author: " Quynh Lam QTL83 SDS348 Fall 2019"
date: '11/26/19'
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
library(Stat2Data)
data("FaithfulFaces")
library(sandwich)
library(lmtest)
library(ggplot2)
library(interactions)
library(MASS)
library(glmnet)
```

# Modeling

## Instructions

A knitted R Markdown document (as a PDF) and the raw R Markdown file (as .Rmd) should both be submitted to Canvas by 11:59pm on 11/24/2019. These two documents will be graded jointly, so they must be consistent (i.e., donâ€™t change the R Markdown file without also updating the knitted document). Knit an html copy too, for later! In the .Rmd file for Project 2, you can copy the first code-chunk into your project .Rmd file to get better formatting. Notice that you can adjust the opts_chunk$set(...) above to set certain parameters if necessary to make the knitting cleaner (you can globally set the size of all plots, etc). I have gone ahead and set a few for you (such as disabling warnings and package-loading messges when knitting)! 

Like before, I envision your written text forming something of a narrative structure around your code/output. All results presented must have corresponding code. Any answers/results/plots etc. given without the corresponding R code that generated the result will not be graded. Furthermore, all code contained in your final project document should work properly. Please do not include any extraneous code or code which produces error messages. (Code which produces warnings is acceptable, as long as you understand what the warnings mean).

## Find data:

The dataset I will be using is called FaithfulFaces, where a rater will rate a photograph of a male or female on their attractiveness, trust worthiness, faithfulness, and sex dimorphia. The data also tells you if the person in the photograph is actually a cheater or not. Attractiveness, trustworthiness, faithfulness and sex dimorphia are all numerical variables. Cheater is a binary variable and tells you whether they are a cheater or not. The categorical variables are the face sex and ranker sex which tells you if they are male or female.

## MANOVA Testing

```{r}
man_faith <- manova(cbind(SexDimorph,Attract,Trust,Faithful)~FaceSex,data=FaithfulFaces)
summary(man_faith)

summary.aov(man_faith)

pairwise.t.test(FaithfulFaces$Faithful,FaithfulFaces$FaceSex,p.adj="none")
pairwise.t.test(FaithfulFaces$SexDimorph,FaithfulFaces$FaceSex,p.adj="none")
pairwise.t.test(FaithfulFaces$Trust,FaithfulFaces$FaceSex,p.adj="none")

1-(.95)^16
.05/16
```

Running a manova to test to see if SexDimorph,Attractfulness, trustfulness, and faithfulness had a mean difference against face sex, it seems that there was as the p-value was below 0.05 and was 6.827e-08. After running an manova, an anova was ran to test to see which ones of the variables had a mean difference. Only SexDimorph,Trust, and Faithful were the only ones that had a mean difference while Attract did not as the p-value was 0.4895 and the other p-values were below 0.05. After the anova, a pairwise t test was run for SexDimorph,Trust, and Faithful since those were the only ones that showed to have a mean difference in the anova test. The pairwise t-test showed that all of those variables showed a difference between males and females, as they were below 0.05. 

## Randomized Testing

```{r}
FaithfulFaces%>%group_by(FaceSex)%>%summarize(mean_face=mean(Trust))
4.444061-4.210591

rand_dist<-vector()
for(i in 1:5000){
Faith<-data.frame(sex=sample(FaithfulFaces$FaceSex),Trust=FaithfulFaces$Trust)
rand_dist[i]<-mean(Faith[Faith$sex=="F",]$Trust)-
mean(Faith[Faith$sex=="M",]$Trust)}

mean(rand_dist>0.23347)*2

{hist(rand_dist,main="",ylab="");abline(v=0.23347,col="red")}
```

Using a randomized test, it took a sample of Face sex and pulled it 5,000 times and then taking the mean of females and males of Trust were subtracted to find the difference. After running a randomized test, I then found the p-value of a two-tailed test and it was less than 0.05, as it was 0.0564. I then plotted a histogram to show the null distribution and test statistic. The null hypothesis is that there is no difference in Trust in males and females, and the alternative hypothesis is that there is a difference in Trust in males and females. From the randomized test and finding the p-value, we can reject the null hypothesis. 

## Linear Regression 

```{r}
fit <- lm(Attract~Trust*FaceSex, data=FaithfulFaces)
coef(fit)

interact_plot(fit,Trust,FaceSex,data=FaithfulFaces)

#assumptions
resids<-fit$residuals
fitvals<-fit$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color='red')
ggplot()+geom_histogram(aes(resids), bins=20)
ggplot()+geom_qq(aes(sample=resids))+geom_qq_line()

#robust standard errors
summary(fit)$coef[,1:2] #uncorrected
coeftest(fit, vcov = vcovHC(fit))[,1:2] #corrected
```

After running a linear regression,the coefficent estimates for trust was 0.355250, Face Sex for males was -0.2845297 and the Trust:Face Sex M was 0.1092399. I then plotted Trust vs Attract in males and females and saw that there was a higher interaction in males than females. I then checked assumptions, and everything looked normal. 

## Bootstrapped Standard Error

```{r}
boot_face<-FaithfulFaces[sample(nrow(FaithfulFaces),replace=TRUE),]
samp_distn<-replicate(5000, {
boot_face<-FaithfulFaces[sample(nrow(FaithfulFaces),replace=TRUE),]
fit2<-lm(Attract~Trust*FaceSex,data=FaithfulFaces)
coef(fit2)
})
samp_distn%>%t%>%as.data.frame%>%summarize_all(sd)
```


## Logistic Regression

```{r}
#logistic regression
fit2<-glm(Cheater~Attract+Trust+Faithful, data=FaithfulFaces, family="binomial")
coeftest(fit2)

#confusion matrix
prob <- predict(fit2,type="response")
table(predict=as.numeric(prob>.5),truth=FaithfulFaces$Cheater)%>%addmargins

class_diag<-function(prob,truth){
tab<-table(factor(prob>.5,levels=c("FALSE","TRUE")),truth)
acc=sum(diag(tab))/sum(tab)
sens=tab[2,2]/colSums(tab)[2]
spec=tab[1,1]/colSums(tab)[1]
ppv=tab[2,2]/rowSums(tab)[2]
if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
#CALCULATE EXACT AUC
ord<-order(prob, decreasing=TRUE)
prob <- prob[ord]; truth <- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup<-c(prob[-1]>=prob[-length(prob)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
data.frame(acc,sens,spec,ppv,auc)
}


class_diag(prob,FaithfulFaces$Cheater)
#ROCplot
library(plotROC)

RocPlot <- ggplot(FaithfulFaces)+geom_roc(aes(d=Cheater,m=prob),n.cut=0)
RocPlot
calc_auc(RocPlot)


#10-fold
set.seed(1234)
k=10

data1<-FaithfulFaces[sample(nrow(FaithfulFaces)),] #randomly order rows
folds<-cut(seq(1:nrow(FaithfulFaces)),breaks=k,labels=FALSE) #create folds
diags<-NULL
for(i in 1:k){
## Create training and test sets
train<-data1[folds!=i,]
test<-data1[folds==i,]
truth<-test$Cheater
## Train model on training set
fit3<-glm(Cheater~.,data=FaithfulFaces,family="binomial")
probs2<-predict(fit3,newdata = test,type="response")
## Test model on test set (save all k results)
diags<-rbind(diags,class_diag(probs2,truth))
}
apply(diags,2,mean)
```

A logistic regression was run and this time the intercepts tell us the interactions of attractfullness and trustfulness in a cheater. The intercepts for attract was -0.18216, trust was 0.11510, and faithful was -0.33650. The accuracy (0.7058824), sensitivity (0), specificity (1), and auc (0.5633333) was also computed. The auc is not as good since it is so low. After running the 10-fold, the auc seemed to stay the same and there was no difference.

## LASSO

```{r}
x <- model.matrix(fit)[,-1]

y<-as.matrix(FaithfulFaces$Cheater)

cv<-cv.glmnet(x,y,family='binomial')

lasso1<-glmnet(x,y,family='binomial',lambda=cv$lambda.1se)
coef(lasso1)

set.seed(1234)
k=10

data2<-FaithfulFaces[sample(nrow(FaithfulFaces)),]
folds<-cut(seq(1:nrow(FaithfulFaces)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){ # FOR EACH OF 10 FOLDS

train2<-data2[folds!=i,] #CREATE TRAINING SET

test2<-data2[folds==i,] #CREATE TESTING SET
truth2<-test2$Cheater


probs3<- predict(fit,newdata=test2, type="response")

diags<-rbind(diags,class_diag(probs3,truth2)) 
}

apply(diags,2,mean)
```

A lasso regression was done and the auc was actually lower than then when I ran a logistic regression. The auc here is 0.4573660 and above was above 0.50. This was after running the 10-fold as well, and it seemed that the accuracy was not as high when running the logistic regression.












